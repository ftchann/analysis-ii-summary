\subsection{Partielle Ableitung (Partial Derivative)}
\begin{Definition}{Partielle Ableitung}{}
    $f$: $\R^n \rightarrow \R$ an Stelle $a$ nach $x_i$

    \[
        \lim_{h \rightarrow 0} \frac{f(a_1, ..., a_i + h, ..., a_n) - f(a_1, ..., a_i, ..., a_n)}{h} =: \frac{\partial f}{\partial x_i}(a)
    \]

\textbf{Wichtig}: Alle anderen $x_i$ werden als konstante behandelt bei Ableitung.
\end{Definition}

\begin{Definition}{Höhere Ableitung}{}
    $f$ ist in der Klasse $C^1$ wenn $f$ differenzierbar ist und alle partiellen Ableitungen stetig sind.\\
    $f$ ist in der Klasse $C^k$ wenn alle partiellen Ableitungen in der Klasse $C^{k-1}$ sind.
\end{Definition}

\begin{Satz}{Satz von Schwarz}{}
    $f \in C^2$. Dann gilt:

    \[
        \frac{\partial^2 f}{\partial x_i x_j} = \frac{\partial^2 f}{\partial x_j x_i} \forall i, j \in \{1, ..., n\}
    \]
    Gleiches gilt for mehr Variablen.

\end{Satz}


\begin{Definition}{Gradient}{}
    \[
    \nabla f =
        \begin{pmatrix}
            \frac{\partial f}{\partial x_1}\\
            \vdots\\
            \frac{\partial f}{\partial x_n}
        \end{pmatrix}
    \]
    Gradient eines Skalarfeldes: Richtung des steilsten Anstiegs; Betrag: Stärke des Anstiegs.

\end{Definition}

\begin{Definition}{Richtungsableitung}{}
    $f$ heisst an der Stelle $a$ in Richtung $u$ differenzierbar, falls der Grenzwert 
	\[
		\lim_{h\to0}\frac{f(a + hu) -f(a)}{h} =
		\left.\frac{d}{dh}f(a+hu)\right|_{h = 0}
		=: D_vf(a) = J_f \cdot v
	\]
existiert. Ist $||u|| = 1$ (normiert), heisst dieser Grenzwert Richtungsableitung.
\end{Definition}

\begin{Rezept}{Richtungsableitung $D_uf(a)$ existiert:}{} Falls $t \mapsto f(a + tu)$ differenzierbar ist bei $t=0$, dann existiert $D_uf(a)$.\\

	Die Richtungsableitung existiert für \textbf{jede Richtung}, falls $\left.\frac{d}{dh} f(a + hu) \right|_{h=0}$ NICHT von $\varphi$ abhängt (wenn mal $x=r\cos(\varphi)$, $y=r\sin(\varphi)$ substituiert).
\end{Rezept}

\begin{Rezept}{Richtungsableitung $D_u f(a)$}{}
Falls $f$ in $a$ differenzierbar ist: Für $f$ in Richtung $u$ in Punkt $a$: 1) $u$ normieren: $\tilde{u} = \frac{u}{||u||}$ 2) Jakobi Matrix berechnen berechnen, dann:

\[
    D_u f(a) = J_f(a) \cdot u = <\nabla f, u> 
\]
\end{Rezept}

\begin{Definition}{Hessematrix}{}
\[
    \mathbf{Hess}(f) =
        \begin{pmatrix}
            \frac{\partial^2 f}{\partial x_1^2}&\hdots&\frac{\partial^2 f}{\partial x_1 x_n}\\
            \vdots&\ddots&\vdots\\
            \frac{\partial^2 f}{\partial x_n x_1}&\hdots&\frac{\partial^2 f}{\partial x_n^2}
        \end{pmatrix}
\]
\end{Definition}

\subsection{Taylorpolynome}

\[
    f(x) = f(a) + f'(a)(x-a) + \frac{1}{2} f''(a)(x-a)^2 + \frac{1}{3!} f^{(3)}(a)(x-a)^3 + ...
\]

\textbf{In $\R^2$}: $\Delta x = (x - x_0)$, ~ $\Delta y = (y - y_0)$
\begin{align*}
    \; & f(x, y) =f(x_0, y_0)\\ &+ \frac{\partial f}{\partial x}(x_0,y_0) \Delta x + \frac{\partial f}{\partial y}(x_0,y_0) \Delta y\\
    &+ \frac{1}{2} \left(\frac{\partial^2 f}{\partial x^2}(x_0,y_0) (\Delta x)^2 + 2\frac{\partial^2 f}{\partial x \partial y}(x_0,y_0) \Delta x \Delta y + \frac{\partial^2 f}{\partial y^2}(x_0,y_0) (\Delta y)^2\right)\\
    &+ \frac{1}{3!} \left(\frac{\partial^3 f}{\partial x^3}(x_0,y_0) (\Delta x)^3 + 3\frac{\partial^3 f}{\partial x^2 \partial y}(x_0,y_0) (\Delta x)^2 \Delta y\right.\\
    &\quad\quad\quad\;+ \left.3\frac{\partial^3 f}{\partial x \partial y^2}(x_0,y_0) \Delta x (\Delta y)^2 + \frac{\partial^3 f}{\partial y^3}(x_0,y_0) (\Delta y)^3\right)\\
    & + ...
\end{align*}

\textbf{In $\R^n$}: $\Delta x_i = x_i - x_i^0$
\[
	Tf(x;x_0) = \left.\sum_{n=0}^\infty \frac{1}{n!}\left(\Delta x_1 \frac{\partial}{\partial x_1} + ... + \Delta x_n \frac{\partial}{\partial x_n} \right)^n 
		f(x_1, ..., x_n)\right|_{(x_1^0,...,x_n^0)}
\]

\textbf{In $\R^n$ bis Grad 2}:
\[
    T_2f(x) = f(x_0) + \nabla f(x_0)	 (x-x_0) + \frac{1}{2} (x-x_0)^\top \text{Hess}_f(x_0) (x-x_0) 
\]
\begin{Satz}{Taylorpolynom}{}
    Sei $k > 1$ eine ganze Zahl, $X$ offen und $f \in C^k$. Für $x_0 \in X$ definieren wir $E_kf(x;x_0) = T_kf(x - x_0; x) + E_kf(x;x_0)$
    dann gilt $
    \displaystyle\lim_{x \rightarrow x_0 \atop x\neq x_0}\frac{E_kf(x;x_0)}{||x-x_0||^k} =0$ 
\end{Satz}
\begin{Definition}{Tangentialebene}{}
    Sei $X \subseteq \mathbb{R}^n$ und $f: X \rightarrow \mathbb{R}^m$ differenzierbar.
    Sei $x_0 \in X$ und $u = df(x_0)$. Der Graph der affinen linearen Approximation
    $g(x) = f(x_0) + u(x - x_0)$ von $\mathbb{R}^n$ nach $\mathbb{R}^m$ is der Tangentialraum an $x_0$ zum Graph von $f$.
\end{Definition}
\begin{Rezept}{Tangentialebene (Variante I)}{}
    Tangentialebene der Fläche $f(x, y)$ finden in Punkt $(x_0, y_0)$.
    
    \textbf{Lösungsschritt I:}
    Erstelle $F(x, y) = (x, y, f(x, y))^\top$ und berechne die
    Basisvektoren für die Tangentialebene
    \[
        dF(x_0, y_0) =
            \begin{pmatrix}
                \frac{\partial F_1}{\partial x}(x_0, y_0)&\frac{\partial F_1}{\partial y}(x_0, y_0)\\
                \frac{\partial F_2}{\partial x}(x_0, y_0)&\frac{\partial F_2}{\partial y}(x_0, y_0)\\
                \frac{\partial F_3}{\partial x}(x_0, y_0)&\frac{\partial F_3}{\partial y}(x_0, y_0)
            \end{pmatrix} = 
            \begin{pmatrix}
                u_1&v_1\\
                u_2&v_2\\
                u_3&v_3
            \end{pmatrix}
    \]
    ($x_0$ und $y_0$ einsetzen).\\
    \textbf{Lösungsschritt II:}
    Berechne Normalvektor:
    \[
        n =
        u \times v =
        \begin{pmatrix}
            u_1\\
            u_2\\
            u_3
        \end{pmatrix}
        \times
        \begin{pmatrix}
            v_1\\
            v_2\\
            v_3
        \end{pmatrix} = 
        \begin{pmatrix}
            a\\
            b\\
            c
        \end{pmatrix}
    \]
    Berechne $d$ mit $p=(x_0,y_0,f(x_0,y_0))$:
    \[
        d=p \cdot n
    \]
    \textbf{Lösungsschritt III:} Konstruiere Gleichung, sodass:
    \[
        ax + by + cz = d
    \]
\end{Rezept}

\begin{Rezept}{Tangentialebene (Variante II)}{}
\textbf{Idee}: Annäherung von $f$ im Punkt $(x_0, y_0)$ durch Taylorpolynom I. Grades
\[ z = f(x_0, y_0) + \frac{\partial f}{\partial x}(x_0, y_0)\cdot(x-x_0) + \frac{\partial f}{\partial y}(x_0, y_0)\cdot(y-y_0) \]
kann direkt umgeformt werden in die Normalform der Ebenengleichung.
\end{Rezept}

\begin{Rezept}{Variablenwechsel}{}
    Sei $g: U \rightarrow X$ und $f: X \rightarrow \mathbb{R}$ und $h = f \circ g: U \rightarrow \mathbb{R}$\\
    $
    \partial_{y_1}h = \frac{\partial f}{\partial x_1}\frac{\partial g_1}{\partial y_1} + ... \ +  \frac{\partial f}{\partial x_n}\frac{\partial g_n}{\partial y_1}$
\end{Rezept}

\begin{Definition}{Variablenwechsel für Inverse}{}
    Sei $X$ offen und $f$ eine differenzierbare Funktion. $f$ ist ein Varaiblenwechsel um $x_0$ wenn es ein Radius $r > 0$ gibt so dass,
    $B = \{x \in \mathbb{R}^n: || x - x_0|| < r\}$ die Eigenschaft hat, dass das Bild $Y = f(B)$ ist offen in $\mathbb{R}^n$
    und $g: Y \rightarrow B$ is differenzierbar mit $f \circ g = Id_Y$ und $g \circ f = Id_B$
\end{Definition}

\begin{Satz}{Inverse Funktion}{}
    Sei $X$ offen und $f$ eine differenzierbare Funktion und $det(J_f(x_0) \neq 0)$, dann ist $f$ ein Variablenwechsel um $x_0$
    und $J_g(f(x_0)) = J_f(x_0)^{-1}$, zusätzlich gilt $f \in C^k \implies g \in C^k$
\end{Satz}